{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuaNgVjJ5bZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcadb5eb-b9ee-4d04-8dd2-ae0bfe0c0a62"
      },
      "source": [
        "from __future__ import absolute_import, division, unicode_literals, print_function\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except exception:\n",
        "  pass  \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re \n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxJp7ti5-MK2"
      },
      "source": [
        "#ABOVE WE DO INTIALISATION PROCESS\n",
        "* IMPORTING TENSORFLOW AND OTHER IMPORTANT LIBRARIES\n",
        "* WORK OF MATPLOTLIB IS SHOWN BELOW\n",
        "\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.arange(4)\n",
        "\n",
        "money = [1.5e5, 2.5e6, 5.5e6, 2.0e7]\n",
        "\n",
        "\n",
        "def millions(x, pos):\n",
        "\n",
        "    'The two args are the value and tick position'\n",
        "    return '$%1.1fM' % (x*1e-6)\n",
        "\n",
        "formatter = FuncFormatter(millions)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.yaxis.set_major_formatter(formatter)\n",
        "\n",
        "plt.bar(x, money)\n",
        "\n",
        "plt.xticks(x + 0.5, ('Bill', 'Fred', 'Mary', 'Sue'))\n",
        "\n",
        "plt.show()\n",
        "![alt text](https://i.stack.imgur.com/q7rse.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Y1zmxdAoNF"
      },
      "source": [
        "# PREPARING DATASET\n",
        "*  downloading launguage dataset from http://www.manythings.org/anki/.\n",
        "**PREPROCESSING STEPS**\n",
        "* add start(sos) and end(eos) token in each sentence\n",
        "* clean sentences by removing special characters\n",
        "* creating word_index and rev. word_index(dict mapping from word->id and id->word)\n",
        "* pad each sentence to threshold\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bdeaff66-90cd-44e2-bc37-0494511bf065"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evmKIcakGlSR",
        "outputId": "8c4bceff-9a84-4257-d7ec-eacdef0dbb83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(path_to_file)\n",
        "print(path_to_zip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/spa-eng/spa.txt\n",
            "/root/.keras/datasets/spa-eng.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcivkmQF-Yg"
      },
      "source": [
        "EXAMPLE OF SMALL VOCABULARY DEVELOPED FROM RAW DATASET\n",
        "\n",
        "![alt text](https://miro.medium.com/max/255/1*_Pp0bAv3nZPYHbPFlvO7Hg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMF8mBwGYiE"
      },
      "source": [
        "#converts unicode to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.rstrip().strip()\n",
        "\n",
        "  #adding sos and eos token so that model know when to start and when to stop\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opI2GzOt479E",
        "outputId": "1aef1195-bee5-4562-a6f8-55fea9813d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvTrtNKkOz8T"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "\n",
        "    #read txt file and split it with spaces\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    #make word pairs of processed data\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    # below zip function returns tuple where each ith element is ith iterable argument\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP7E99KzFj2x",
        "outputId": "f5a4b00c-6d05-4562-ac32-d39c1527e6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "en, sp = create_dataset(path_to_file,None)\n",
        "print(en[100])\n",
        "print(sp[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> go away ! <end>\n",
            "<start> salga de aqui ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1hXJQ_-UNTX"
      },
      "source": [
        "#this function returns MAX LENGTH of sentence present in database returned length is further used \n",
        "#to set threshold for input size shorter sentences are post padded\n",
        "def max_length(tensor):\n",
        "  return(max(len(t) for t in tensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlVvZvKBU-2T"
      },
      "source": [
        "# TOKENIZATION \n",
        "* First step of converting are datasets or rather mapping of words to certain value(word-value pair) is done above as text data is not directly fed to neural networks these pair are known as 'Integer Tokens'  \n",
        "*  In second step these integer tokens are converted word vectors consisting of floating point numbers thus forming embedded layers\n",
        "![alt text](https://freecontent.manning.com/wp-content/uploads/Chollet_DLfT_01.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAIBfJvgXDpq"
      },
      "source": [
        "def tokenize(language):\n",
        "  #filters argument is used to extract string where each element is character that we will\n",
        "  #be filtered from texts it includes punctuation line breaks tabs and exclude ' ' '\n",
        "  language_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  \n",
        "  #THIS FUNCTION UPDATES INTERNAL LIBRARY ON BASIS OF TEXTS eg:- for t something like, \"The cat sat on the mat.\" \n",
        "  #It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2,, 0 is reserved for padding\n",
        "  language_tokenizer.fit_on_texts(language)\n",
        "\n",
        "  #TRANSFORM EACH TEXT TO SEQUENCE OF INTEGERS\n",
        "  tensor = language_tokenizer.texts_to_sequences(language)\n",
        "\n",
        "  #PERFORM POST PADDING ON TEXT BASED ON THRESHOLD DEFINED BY MAX LEN OF TENSOR \n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor,language_tokenizer \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlXDBNUrFIfK"
      },
      "source": [
        "def load_dataset(path,num_examples=None):\n",
        "  #TO CREATE CLEANED OUTPUT AND INPUT PAIR\n",
        "  target_language, input_language = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, input_language_tokenizer = tokenize(input_language)\n",
        "\n",
        "  target_tensor, target_language_tokenizer = tokenize(target_language)\n",
        "  \n",
        "  return input_tensor, target_tensor, input_language_tokenizer, target_language_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7nlpqnugfj4"
      },
      "source": [
        "# TRAIN/VALIDATION SPLIT\n",
        "* Training on complete dataset of *122936* sentences(trying to push its limits)\n",
        "* Train-Test split of 80-20 ratio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbCpEz_lCEnT"
      },
      "source": [
        "num_examples = 122936\n",
        "input_tensor , target_tensor, input_language, target_language = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "#max length of target tensors\n",
        "max_length_target, max_length_input = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "#creating 80-20 split of training and validation sets\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7iXxwP4K5XL",
        "outputId": "15bc1720-8dde-4b68-c22e-0fd1a1025f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#printing Train/val data\n",
        "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95171 23793 95171 23793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3fXDBAsLiEn"
      },
      "source": [
        "**SHOWING INDEX TO WORD MAPPINGS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FMHv9DSLut3"
      },
      "source": [
        "def convert(language,tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(\"%d ----> %s\" % (t, language.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv8VfGiAMevX",
        "outputId": "a3a9a38b-d67a-410b-e1dd-1c40f0e0179a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "print(\"Input Language: index to word mapping\")\n",
        "convert(input_language,input_tensor_train[20])\n",
        "print()\n",
        "print(\"target language: index to word mapping\")\n",
        "convert(target_language, target_tensor_train[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language: index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> no\n",
            "17 ----> me\n",
            "8347 ----> interesaba\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "target language: index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "253 ----> wasn\n",
            "12 ----> t\n",
            "681 ----> interested\n",
            "16 ----> in\n",
            "14 ----> it\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veCJVYISRu2W"
      },
      "source": [
        "# CREATING DATASET BATCH\n",
        "* Word embeddings visualisation\n",
        "![alt text](https://miro.medium.com/max/990/1*Fat62b1ZITOFMPXTcHNkLw.jpeg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox3RPcMcR_yx"
      },
      "source": [
        "#BUFFER SIZE IS USED AS AN ARGUMENT TO SHUFFLE FUCNTION\n",
        "#If buffer size is 100, it means that Tensorflow will keep a buffer of the next \n",
        "#100 samples, and will randomly select one those 100 samples. it then adds the next element to the buffer.\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "#batch size is input mini batch if equal to whole input size batch then it would tend to gd resulting in high acc. but very slow iteration\n",
        "#whereas if batch size=1 then sgd would apply on fast but due to singular direction of gd loss jumps around   \n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#steps per epoch is operation performed by model per epoch also here instead of random intialisation we obtain it via floor function\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "#embedding layer dimension\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_input_size = len(input_language.word_index)+1\n",
        "vocab_target_size = len(target_language.word_index)+1\n",
        "\n",
        "#DEFINING TENSORFLOW'S INBUILT DYNAMIC DATA PIPELINE FOR INPUT USING TENSOR SLICES \n",
        "#NOTE:- we use .tensor_slices instead .tensor coz former retrns word index as elements but latter returns list of sublists of elements(in slices form)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "#for our gpu/tpu to get excatly same size of batch we enable drop remainder = true such that any partial batch is dropped\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIp2v06ovIMb",
        "outputId": "421bcd2e-b215-48c8-d6d5-edd5b9330bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 53]), TensorShape([128, 51]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss2kOdMgvKe2"
      },
      "source": [
        "# **MODEL ARCHITECTURE**\n",
        "**INSPIRED BY NEURAL MACHINE TRANSLATION\n",
        "BY JOINTLY LEARNING TO ALIGN AND TRANSLATE(D.Bahdanau)**\n",
        "\n",
        "\n",
        "**BIDIRECTIONAL RECURRENT NEURAL NETWORK ARE USED HERE**\n",
        "![alt text](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
        "\n",
        "**GRU CELLS**\n",
        "\n",
        "![alt text](https://blog.floydhub.com/content/images/2019/07/image14.jpg)\n",
        "\n",
        "\n",
        "**ENCODER-DECODER MODEL WITH ATTENTION MECHANISM IS IMPLEMENTED**\n",
        "![alt text](https://devblogs.nvidia.com/wp-content/uploads/2015/07/Figure5_attention_3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdHq8fMw7gTH"
      },
      "source": [
        "#**ENCODER CLASS**\n",
        "* The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size).\n",
        "* Batch_size= 128 , embeddings_dim = 250, encoder_units = 1024 \n",
        "\n",
        "INTUTION FOR ENCODER MODEL\n",
        "![alt text](https://miro.medium.com/max/3081/1*xd8j4KoKRSzRq0b1Vx0FAA.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6T2oOd8vPfC"
      },
      "source": [
        "#defining our encoders architecture\n",
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embeddings_dim, encoder_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    self.encoder_units = encoder_units\n",
        "    self.gru = tf.keras.layers.GRU(self.encoder_units,\n",
        "                                  return_sequences = True,\n",
        "                                  return_state = True,\n",
        "                                  recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output,state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.encoder_units))  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOX_V6ipw-Au",
        "outputId": "d0146e08-cc3b-4e2a-903e-01eb47cbcd43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (128, 53, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (128, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uckuano6Zgq"
      },
      "source": [
        "# ATTENTION MECHANISM\n",
        "**USING BAHDANHAU ATTENTION**\n",
        "![alt text](https://jscriptcoder.github.io/date-translator/attn_mechanism.png)\n",
        "![alt text](https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg)![alt text](https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9vCmOT36dql"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    \n",
        "    # W & U are weights for current encoder output and previous hidden state \n",
        "    # V is weights for feed forward nn used in attention mechanism\n",
        "    self.W1 =tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    #decorator call\n",
        "\n",
        "    #hidden_with_time_axis == (batch_size, 1, hidden size)\n",
        "    #query compresses previous output into m dimension(here m=1)\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    #score shape == (batch_size, max_length, 1)\n",
        "    #we get 1 at last axis coz we r applying score to self.V\n",
        "    #shape of tensor before applying to self.v is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values)+self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    #attention weights shape(batch_size, max_length,1)\n",
        "    attention_weights = tf.nn.softmax(score,axis=1)\n",
        "\n",
        "    #context vector shape after is (batch_size, hidden_size)\n",
        "    #context vector is summed using tf.reduce_sum axis=1 is taking element batch along column\n",
        "    context_vector = attention_weights*score\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "     \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GqosfdER9fC",
        "outputId": "68fbdad6-8313-4f54-9353-4aee1684cb96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (128, 1)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 53, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOhShpgVSENg"
      },
      "source": [
        "# **DECODER CLASS**\n",
        "* At each time step previous output ,  hidden state and context vector is INPUT TO DECODER  \n",
        "* Output is predicted translated word\n",
        "* Batch_size = 128 , decoder_units = 1024, embeddings_dim = 250\n",
        "\n",
        "INTUTION FOR DECODER MODEL\n",
        "![alt text](https://miro.medium.com/max/1516/1*GwKpF9yMipPWuruXoTWKPQ.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOni-Ds7SBp_"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    #same architecture as encoder just the addition of single dense layer \n",
        "    self.batch_sz = batch_sz\n",
        "    self.decoder_units = decoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.decoder_units, \n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #used for attention \n",
        "    self.attention = BahdanauAttention(self.decoder_units)\n",
        "  def call(self, x, hidden, encoder_output):\n",
        "    #encoder_output = batch_size, max_length, hidden\n",
        "    context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, hidden_state + embedding_dim)\n",
        "    # context vector and previous state and output is current input to gru layer\n",
        "    x = tf.concat([tf.expand_dims(context_vector,1),x], axis=-1)\n",
        "\n",
        "    #passing concatenated vector to gru\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    \n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "        \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnzkHBkjwDXl",
        "outputId": "fed60515-2ece-4455-fbe3-e7254e0443b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((128, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 12934)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wajYvCv35gz"
      },
      "source": [
        "#OPTIMIZER AND LOSS FUNCTION\n",
        "**Adam OPTIMIZER**\n",
        "![alt text](https://miro.medium.com/max/3272/1*YJCqitHcljUpCGf058WOIw.png)\n",
        "![alt text](https://blog.paperspace.com/content/images/2018/06/adam.png)\n",
        "\n",
        "\n",
        "**CROSS-ENTROPY LOSS FUNCTION IS USED**\n",
        "![alt text](https://miro.medium.com/max/778/1*JZ-qea3BYaGOT4Vdhds9mQ.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lGqhUU039br"
      },
      "source": [
        "# selected optimizer is Adam\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#selected loss function is categorical crossentropy for multiclass classification\n",
        "#from logits is set to true \n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, predicted):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, predicted)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzIo1lYt4O-K"
      },
      "source": [
        "checkpoints(object based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHM-NCB4Ufv"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixAC8ZBJ4mYQ"
      },
      "source": [
        "# TRAINING THE MODEL\n",
        "NOTE :- FOR MORE ROBUSTNESS OF MODEL WE IMPLEMENT TEACHER FORCING ALGORITHM IN DECODER \n",
        "Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network.\n",
        "![alt text](https://i.ytimg.com/vi/fAQ-yV__168/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXqcUPnt4oYf"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target, encoder_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, encoder_hidden = encoder(inp, encoder_hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoder_input = tf.expand_dims([target_language.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, target.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "\n",
        "      loss += loss_function(target[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      decoder_input = tf.expand_dims(target[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_O6ERpt4oTa",
        "outputId": "91fa88f2-1d52-4b69-d7f1-07ba12dc8fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  encoder_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, target, encoder_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 10 epochs\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enVeRIOE44jQ"
      },
      "source": [
        "#EVALUATION AND RESULTS\n",
        "FUNCTION FOR TESTING MODEL ON USER INPUTTED DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMvGgJZ47t9"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [input_language.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    encoder_output, encoder_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = tf.expand_dims([target_language.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, decoder_hidden, attention_weights = decoder(decoder_input,\n",
        "                                                             decoder_hidden,\n",
        "                                                             encoder_output)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_language.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_language.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2RH5zG45Bp9"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzVr1kYF5D4K"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM85xBkd5sBy"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFu8HGbA5x8S"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}